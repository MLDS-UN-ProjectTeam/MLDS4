{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06eb2db5",
      "metadata": {
        "id": "06eb2db5"
      },
      "source": [
        "<img src=\"./resources/images/banner3.png\" width=\"100%\" alt=\"Encabezado MLDS\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb006272",
      "metadata": {
        "id": "fb006272"
      },
      "source": [
        "# **Extracción de Características**\n",
        "---\n",
        "\n",
        "## **0. Integrantes del equipo de trabajo**\n",
        "---\n",
        "\n",
        "<table><thead>\n",
        "  <tr>\n",
        "    <th>#</th>\n",
        "    <th>Integrante</th>\n",
        "    <th>Documento de identidad</th>\n",
        "  </tr></thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Ivonne Cristina Ruiz Páez</td>\n",
        "    <td>1014302058</td>  \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>Diego Alejandro Feliciano Ramos</td>\n",
        "    <td>1024586904</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3</td>\n",
        "    <td>Cristhian Enrique Córdoba Trillos</td>\n",
        "    <td>1030649666</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5bb0b6",
      "metadata": {
        "id": "cd5bb0b6"
      },
      "source": [
        "## **1. Selección del Embedding**\n",
        "---\n",
        "\n",
        "Para el análisis de sentimientos sobre un corpus de 50.000 documentos en inglés provenientes de redes sociales, se seleccionó la técnica de embedding FastText por su capacidad de generar representaciones distribuidas de palabras que incorporan información subléxica a través del modelado de n-gramas de caracteres. A diferencia de Word2Vec, que representa cada palabra como un vector independiente, FastText descompone las palabras en subcomponentes, lo cual permite manejar de forma más robusta errores ortográficos, abreviaciones y palabras fuera del vocabulario (OOV), fenómenos frecuentes en el lenguaje informal y no estructurado característico de las plataformas sociales. Esta propiedad resulta especialmente útil para capturar de manera más precisa el contenido semántico de los textos breves y ruidosos. Además, su eficiencia computacional lo hace adecuado para corpus de tamaño medio-grande como el presente, permitiendo generar embeddings útiles para tareas de clasificación sin incurrir en los altos costos de cómputo de modelos basados en transformers. Si bien FastText no produce embeddings contextuales, su generalización a partir de subpalabras ofrece una mejora significativa sobre técnicas tradicionales como TF-IDF o Bag-of-Words, posicionándolo como una alternativa balanceada en términos de precisión y escalabilidad para tareas de análisis de sentimientos en dominios informales como las redes sociales."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fef6dbc",
      "metadata": {
        "id": "3fef6dbc"
      },
      "source": [
        "## **2. Implementación del Embedding**\n",
        "---\n",
        "\n",
        "Implemente la estrategia de embedding a partir del conjunto de datos pre-procesado. Recuerde que:\n",
        "\n",
        "- `sklearn`: permite implementar bolsas de palabras, TF-IDF y bolsas de N-grams a partir del módulo `sklearn.feature_extraction.text`.\n",
        "- `gensim`: permite implementar word2vec, fasttext y doc2vec desde `gensim.models`.\n",
        "- `spacy`: permite representar textos con embeddings pre-entrenados con el atributo `vector`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34dd66cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34dd66cf",
        "outputId": "2694fe23-b4e7-4c8d-860d-c05b61842e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m394.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=792ee4263df5d3941805ac8693a4e328d497a6b54e21194de14ba200155869b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: pyahocorasick, numpy, langdetect, emoji, anyascii, textsearch, scipy, gensim, contractions\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anyascii-0.3.2 contractions-0.1.73 emoji-2.14.1 gensim-4.3.3 langdetect-1.0.9 numpy-1.26.4 pyahocorasick-2.1.0 scipy-1.13.1 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "%pip install textblob kagglehub emoji gensim pandas numpy wordcloud contractions langdetect tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "79zNjXR6JgEV"
      },
      "id": "79zNjXR6JgEV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "630062b1",
      "metadata": {
        "id": "630062b1"
      },
      "outputs": [],
      "source": [
        "# Importa módulos para expresiones regulares, manejo de tiempo, registro de eventos, y procesamiento paralelo\n",
        "#\n",
        "import re\n",
        "import time\n",
        "import logging\n",
        "import multiprocessing as mp\n",
        "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
        "\n",
        "# Importa tqdm para mostrar barras de progreso\n",
        "#\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Importa kagglehub para interactuar con Kaggle y pandas para manipulación de datos\n",
        "#\n",
        "import nltk\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "\n",
        "# Importa módulos para procesamiento de texto y detección de idioma\n",
        "#\n",
        "from emoji import demojize\n",
        "import contractions\n",
        "from langdetect import detect, DetectorFactory\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import TextBlob\n",
        "from textblob import download_corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53b55fdb",
      "metadata": {
        "id": "53b55fdb"
      },
      "outputs": [],
      "source": [
        "# Función para detectar el idioma de un texto\n",
        "def detectar_idioma(texto):\n",
        "    try:\n",
        "        return detect(str(texto))\n",
        "\n",
        "    except:\n",
        "        return \"unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "981bb032",
      "metadata": {
        "id": "981bb032"
      },
      "outputs": [],
      "source": [
        "# Ruta del archivo CSV de salida\n",
        "OUTPUT_CSV = \"./dataset_limpio.csv\"\n",
        "\n",
        "# Nombre de la columna de texto en el dataset\n",
        "TEXT_COLUMN = \"statement\"\n",
        "\n",
        "# Tiempo máximo de espera en segundos\n",
        "TIMEOUT_SEGUNDOS = 5\n",
        "\n",
        "# Flag opcional para activar corrección ortográfica\n",
        "CORREGIR_ORTOGRAFIA = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e15d8f8d",
      "metadata": {
        "id": "e15d8f8d",
        "outputId": "9656e3a9-be70-4731-b567-60d2744d8131"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "def limpiar_texto(texto):\n",
        "    # 0. Asegurar que sea string\n",
        "    texto = str(texto)\n",
        "\n",
        "    # 1. Expandir contracciones (\"I'm\" -> \"I am\")\n",
        "    texto = contractions.fix(texto)\n",
        "\n",
        "    # 2. Demojizar\n",
        "    texto = demojize(texto)\n",
        "\n",
        "    # 3. Reemplazar URLs, menciones, hashtags\n",
        "    texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", texto)\n",
        "    texto = re.sub(r\"@\\w+\", \" \", texto)\n",
        "    texto = re.sub(r\"#(\\w+)\", r\"\\1\", texto)\n",
        "\n",
        "    # 4. Reemplazar puntuación por espacios\n",
        "    texto = re.sub(r\"[^a-zA-Z]\", \" \", texto)\n",
        "\n",
        "    # 5. Minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # 6. Tokenización\n",
        "    tokens = word_tokenize(texto)\n",
        "\n",
        "    # 7. Filtro de tokens\n",
        "    tokens_filtrados = [\n",
        "        token for token in tokens\n",
        "        if token not in stop_words and len(token) > 1 and token.isalpha()\n",
        "    ]\n",
        "\n",
        "    # 8. Corrección ortográfica opcional\n",
        "    if CORREGIR_ORTOGRAFIA:\n",
        "        tokens_corregidos = []\n",
        "        for token in tokens_filtrados:\n",
        "            palabra_corregida = str(TextBlob(token).correct())\n",
        "            tokens_corregidos.append(palabra_corregida)\n",
        "    else:\n",
        "        tokens_corregidos = tokens_filtrados\n",
        "\n",
        "    # 9. Lematización\n",
        "    tokens_lemmatizados = [lemmatizer.lemmatize(token) for token in tokens_corregidos]\n",
        "\n",
        "    # 10. Retornar lista final de tokens\n",
        "    return tokens_lemmatizados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9631e38",
      "metadata": {
        "id": "d9631e38"
      },
      "outputs": [],
      "source": [
        "# Ejecuta la función de limpieza de texto con un límite de tiempo\n",
        "#\n",
        "def timeout_en_limpieza(texto):\n",
        "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
        "        futuro = executor.submit(limpiar_texto, texto)\n",
        "        try:\n",
        "            return futuro.result(timeout=TIMEOUT_SEGUNDOS)\n",
        "        except TimeoutError:\n",
        "            logging.warning(\"⏱ Timeout en limpieza: \" + str(texto[:40] + \"\\n\"))\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Error al limpiar texto: {texto[:40]}... -> {str(e)} \\n\")\n",
        "            return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44192db1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44192db1",
        "outputId": "5752542f-d0fa-436e-a152-543da80c4508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/sentiment-analysis-for-mental-health\n"
          ]
        }
      ],
      "source": [
        "# Descarga la última versión del dataset - Kaggle\n",
        "path = kagglehub.dataset_download(\"suchintikasarkar/sentiment-analysis-for-mental-health\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "# Carga la data del dataset\n",
        "path = '/Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1/Combined Data.csv'\n",
        "df = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efdfe567",
      "metadata": {
        "id": "efdfe567"
      },
      "outputs": [],
      "source": [
        "# Procesa una lista de textos en paralelo utilizando múltiples procesos\n",
        "#\n",
        "def procesar_en_paralelo(funcion, lista_textos, num_procesos=None):\n",
        "    if num_procesos is None:\n",
        "        from os import cpu_count\n",
        "        num_procesos = cpu_count()\n",
        "    with mp.Pool(processes=num_procesos) as pool:\n",
        "        resultados = list(tqdm(pool.imap(funcion, lista_textos), total=len(lista_textos)))\n",
        "    return resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e5bd2b",
      "metadata": {
        "id": "35e5bd2b"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    try:\n",
        "        # Inicia el procesamiento y carga los datos\n",
        "        logging.info(\"🚀 Inicio del procesamiento\")\n",
        "        print(\"📦 Cargando datos...\")\n",
        "        print(f\"✅ {len(df)} documentos cargados.\")\n",
        "        logging.info(f\"{len(df)} documentos cargados.\")\n",
        "\n",
        "        # Detecta el idioma de los documentos\n",
        "        print(\"🌍 Detectando idioma...\")\n",
        "        df[\"lang\"] = df[TEXT_COLUMN].apply(detectar_idioma)\n",
        "        df_ingles = df[df[\"lang\"] == \"en\"].copy()\n",
        "        print(f\"✅ Documentos en inglés: {len(df_ingles)}\")\n",
        "\n",
        "        # Procesa los textos en paralelo con límite de tiempo por fila\n",
        "        print(\"🧠 Procesando en paralelo con timeout por fila...\")\n",
        "        textos = df_ingles[TEXT_COLUMN].tolist()\n",
        "        textos_limpios = procesar_en_paralelo(timeout_en_limpieza, textos, num_procesos=11)\n",
        "\n",
        "        # Guarda los resultados en un archivo .CSV\n",
        "        print(\"💾 Guardando resultados...\")\n",
        "        df[\"clean_tokens\"] = textos_limpios\n",
        "        df.to_csv(OUTPUT_CSV, index=False)\n",
        "        logging.info(f\"Archivo guardado como: {OUTPUT_CSV}\")\n",
        "        print(f\"✅ Archivo guardado como: {OUTPUT_CSV}\")\n",
        "    except Exception as e:\n",
        "        # Maneja errores durante el procesamiento\n",
        "        print(\"❌ Error durante el procesamiento:\")\n",
        "        print(e)\n",
        "        logging.error(f\"Error general del script: {str(e)}\")\n",
        "    finally:\n",
        "        # Calcula y muestra el tiempo total de procesamiento\n",
        "        end = time.time()\n",
        "        elapsed = (end - start) / 60\n",
        "        print(f\"⏱ Tiempo total: {elapsed:.2f} minutos\")\n",
        "        logging.info(f\"Tiempo total: {elapsed:.2f} minutos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c04a27b",
      "metadata": {
        "id": "1c04a27b",
        "outputId": "80220ae4-6955-434f-a3e2-aa57a8e42c9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "📦 Cargando datos...\n",
            "✅ 53043 documentos cargados.\n",
            "🌍 Detectando idioma...\n",
            "✅ Documentos en inglés: 49832\n",
            "🧠 Procesando en paralelo con timeout por fila...\n",
            "  0%|                                                 | 0/49832 [00:00<?, ?it/s][nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            " 11%|████▏                                 | 5468/49832 [00:45<15:16, 48.41it/s]WARNING:root:⏱ Timeout en limpieza: 19 years old, male, from the Philippines\n",
            "\n",
            " 13%|████▊                                 | 6261/49832 [01:11<25:27, 28.53it/s]WARNING:root:⏱ Timeout en limpieza: So introduction I guess.. my name is Mic\n",
            "\n",
            " 13%|████▌                               | 6269/49832 [01:15<1:10:56, 10.23it/s]WARNING:root:⏱ Timeout en limpieza: I miss my children,They are the purest f\n",
            "\n",
            " 16%|██████                                | 7893/49832 [02:01<21:31, 32.48it/s]WARNING:root:⏱ Timeout en limpieza: Hello everyone,I rarely post on Reddit b\n",
            "\n",
            " 18%|██████▋                               | 8726/49832 [02:29<25:55, 26.43it/s]WARNING:root:⏱ Timeout en limpieza: Venting. This is long, sorry. Feel free \n",
            "\n",
            " 18%|██████▉                               | 9160/49832 [02:43<22:07, 30.64it/s]WARNING:root:⏱ Timeout en limpieza: I have only 1 person I can somewhat open\n",
            "\n",
            " 19%|███████                               | 9233/49832 [02:51<43:22, 15.60it/s]WARNING:root:⏱ Timeout en limpieza: Originally posted on the r/suboxone, I t\n",
            "\n",
            " 20%|███████▌                              | 9970/49832 [03:08<18:56, 35.07it/s]WARNING:root:⏱ Timeout en limpieza: I no longer know what else to do but wri\n",
            "\n",
            " 20%|███████▌                              | 9978/49832 [03:12<43:48, 15.16it/s]WARNING:root:⏱ Timeout en limpieza: And has life gotten better?&amp;#x200B;N\n",
            "\n",
            " 21%|███████▌                             | 10231/49832 [03:18<20:08, 32.76it/s]WARNING:root:⏱ Timeout en limpieza: I am not fucking exaggerating i do not h\n",
            "\n",
            " 23%|████████▍                            | 11409/49832 [03:52<16:07, 39.73it/s]WARNING:root:⏱ Timeout en limpieza: gross gross gross gross gross gross gros\n",
            "\n",
            " 25%|█████████▏                           | 12300/49832 [04:25<43:22, 14.42it/s]WARNING:root:⏱ Timeout en limpieza: Ok. I am not a short-winded person, so I\n",
            "\n",
            " 25%|█████████▎                           | 12523/49832 [04:31<22:14, 27.97it/s]WARNING:root:⏱ Timeout en limpieza: I wrote a song about how painful it is t\n",
            "\n",
            " 26%|█████████▍                           | 12785/49832 [04:39<21:27, 28.78it/s]WARNING:root:⏱ Timeout en limpieza: Hello, before I start, I am not a native\n",
            "\n",
            " 27%|█████████▊                           | 13213/49832 [04:53<24:21, 25.06it/s]WARNING:root:⏱ Timeout en limpieza: Yeah, this is not the first, or even sec\n",
            "\n",
            " 29%|██████████▋                          | 14440/49832 [05:28<26:04, 22.62it/s]WARNING:root:⏱ Timeout en limpieza: it is a terrible idea for one diagnose o\n",
            "\n",
            " 33%|████████████▏                        | 16464/49832 [06:28<22:14, 25.01it/s]WARNING:root:⏱ Timeout en limpieza: You cannot runYou cannot hideSounds oh s\n",
            "\n",
            "WARNING:root:⏱ Timeout en limpieza: Dear ***,I am writing to you because I h\n",
            "\n",
            "WARNING:root:⏱ Timeout en limpieza: Pleasekillme. Please. God. ImbeggingyouG\n",
            "\n",
            " 33%|████████████▎                        | 16510/49832 [06:34<40:46, 13.62it/s]WARNING:root:⏱ Timeout en limpieza: Ineededthat. Allthemoreammothatwillhelpm\n",
            "\n",
            "WARNING:root:⏱ Timeout en limpieza: I am apos thatsleepsoutside behindtheMic\n",
            "\n",
            "WARNING:root:⏱ Timeout en limpieza: If anyoneseemesleepingbehind theMichaeld\n",
            "\n",
            "WARNING:root:⏱ Timeout en limpieza: Everynig,ht I hope that when I sle,ep be\n",
            "\n",
            " 36%|█████████████▍                       | 18065/49832 [07:21<13:58, 37.90it/s]WARNING:root:⏱ Timeout en limpieza: I am 25 years old. I have struggled with\n",
            "\n",
            " 38%|██████████████▏                      | 19102/49832 [07:55<14:46, 34.68it/s]WARNING:root:⏱ Timeout en limpieza: i realised last night that I have been i\n",
            "\n",
            " 40%|██████████████▊                      | 19947/49832 [08:18<11:21, 43.85it/s]WARNING:root:⏱ Timeout en limpieza: These are some things that bring me joy.\n",
            "\n",
            " 40%|██████████████▊                      | 19973/49832 [08:25<38:09, 13.04it/s]WARNING:root:⏱ Timeout en limpieza: I constantly repeat to myself that I hav\n",
            "\n",
            " 41%|███████████████▎                     | 20554/49832 [08:36<12:34, 38.82it/s]WARNING:root:⏱ Timeout en limpieza: I do not expect anyone to read this ramb\n",
            "\n",
            " 42%|███████████████▍                     | 20750/49832 [08:43<14:44, 32.89it/s]WARNING:root:⏱ Timeout en limpieza: Do I, a privileged, loved, white woman r\n",
            "\n",
            " 43%|███████████████▊                     | 21376/49832 [08:59<14:02, 33.78it/s]WARNING:root:⏱ Timeout en limpieza: For reference, I am a 22 year old you.S \n",
            "\n",
            " 44%|████████████████▍                    | 22152/49832 [09:22<17:16, 26.72it/s]WARNING:root:⏱ Timeout en limpieza: I need support or encouragement. I (29M)\n",
            "\n",
            " 44%|████████████████▍                    | 22158/49832 [09:26<59:13,  7.79it/s]WARNING:root:⏱ Timeout en limpieza: This is a a vent. I (29M) really do not \n",
            "\n",
            " 45%|████████████████▋                    | 22537/49832 [09:38<17:05, 26.61it/s]WARNING:root:⏱ Timeout en limpieza: 7.7.21Should I kill myself?A long journa\n",
            "\n",
            " 46%|█████████████████▏                   | 23108/49832 [09:53<09:57, 44.69it/s]WARNING:root:⏱ Timeout en limpieza: **Following is a boring and overwhelming\n",
            "\n",
            " 46%|█████████████████▏                   | 23117/49832 [09:58<42:43, 10.42it/s]WARNING:root:⏱ Timeout en limpieza: **Following is a boring and overwhelming\n",
            "\n",
            " 46%|█████████████████▏                   | 23155/49832 [09:59<29:07, 15.27it/s]WARNING:root:⏱ Timeout en limpieza: I am feeling like there is no hope in my\n",
            "\n",
            " 47%|█████████████████▌                   | 23579/49832 [10:09<12:52, 34.00it/s]WARNING:root:⏱ Timeout en limpieza: why even bother going on anymore, it is \n",
            "\n",
            " 48%|█████████████████▋                   | 23790/49832 [10:17<15:24, 28.15it/s]WARNING:root:⏱ Timeout en limpieza: *Thank you to the moderators of* r/Suici\n",
            "\n",
            " 51%|██████████████████▉                  | 25424/49832 [11:04<17:06, 23.78it/s]WARNING:root:⏱ Timeout en limpieza: idon't have friends and it hurts being l\n",
            "\n",
            " 51%|██████████████████▉                  | 25515/49832 [11:09<18:17, 22.16it/s]WARNING:root:⏱ Timeout en limpieza: MARTINWhy are not you talking?GEORGIEBec\n",
            "\n",
            " 64%|███████████████████████▊             | 31990/49832 [12:17<04:22, 67.89it/s]WARNING:root:⏱ Timeout en limpieza: Pretty bad anxiety towards taking meds w\n",
            "\n",
            " 66%|████████████████████████▎            | 32679/49832 [12:51<15:52, 18.00it/s]WARNING:root:⏱ Timeout en limpieza: Some very helpful words to put things in\n",
            "\n",
            " 66%|████████████████████████▍            | 32841/49832 [13:00<20:48, 13.61it/s]WARNING:root:⏱ Timeout en limpieza: Does this appear to be anxiety, or somet\n",
            "\n",
            " 73%|███████████████████████████▏         | 36553/49832 [14:34<12:30, 17.69it/s]WARNING:root:⏱ Timeout en limpieza: hello everyone i m an 9 year old male wi\n",
            "\n",
            " 74%|███████████████████████████▌         | 37092/49832 [14:52<05:40, 37.39it/s]WARNING:root:⏱ Timeout en limpieza: a little over a month ago i wa over at a\n",
            "\n",
            " 75%|███████████████████████████▋         | 37219/49832 [14:58<06:36, 31.84it/s]WARNING:root:⏱ Timeout en limpieza: we ve been seeing a worrying increase in\n",
            "\n",
            "WARNING:root:⏱ Timeout en limpieza: it doesn t matter anymore i m going to c\n",
            "\n",
            " 75%|███████████████████████████▋         | 37219/49832 [15:10<06:36, 31.84it/s]WARNING:root:⏱ Timeout en limpieza: i miss jesus i miss the warmth warmth of\n",
            "\n",
            " 76%|████████████████████████████▏        | 37936/49832 [15:20<04:35, 43.16it/s]WARNING:root:⏱ Timeout en limpieza: i have come to the conclusion that i am \n",
            "\n",
            " 86%|███████████████████████████████▋     | 42649/49832 [16:42<10:31, 11.37it/s]WARNING:root:⏱ Timeout en limpieza: [Rant] I KNEW IT! My medication is poiso\n",
            "\n",
            " 86%|███████████████████████████████▉     | 43053/49832 [17:09<09:34, 11.79it/s]WARNING:root:⏱ Timeout en limpieza: as my gujurati jain friend wants scienti\n",
            "\n",
            " 87%|████████████████████████████████     | 43131/49832 [17:14<05:32, 20.18it/s]WARNING:root:⏱ Timeout en limpieza: Medication Frustration (Triiger warning-\n",
            "\n",
            "WARNING:root:⏱ Timeout en limpieza: Have been taking Lexapro 10mg for past 4\n",
            "\n",
            " 88%|████████████████████████████████▌    | 43788/49832 [17:58<09:12, 10.93it/s]WARNING:root:⏱ Timeout en limpieza: DEPRESSION HAS A PURPOSE: HOW TO USE IT \n",
            "\n",
            "WARNING:root:⏱ Timeout en limpieza: Creating A Transcendental Platform For T\n",
            "\n",
            " 89%|████████████████████████████████▊    | 44170/49832 [18:24<04:40, 20.17it/s]WARNING:root:⏱ Timeout en limpieza: A day in the mixed episode from hell...I\n",
            "\n",
            " 89%|████████████████████████████████▊    | 44226/49832 [18:29<05:34, 16.77it/s]WARNING:root:⏱ Timeout en limpieza: A mother’s letter about forgiveness and \n",
            "\n",
            " 90%|█████████████████████████████████▎   | 44817/49832 [18:48<03:19, 25.14it/s]WARNING:root:⏱ Timeout en limpieza: For anyone on the fence about journaling\n",
            "\n",
            " 93%|██████████████████████████████████▎  | 46183/49832 [19:39<02:23, 25.45it/s]WARNING:root:⏱ Timeout en limpieza: Stress and your body's endocannabinoid t\n",
            "\n",
            " 94%|██████████████████████████████████▋  | 46649/49832 [19:58<02:30, 21.11it/s]WARNING:root:⏱ Timeout en limpieza: At this point in life I rarely truly wor\n",
            "\n",
            " 94%|██████████████████████████████████▋  | 46653/49832 [20:01<05:42,  9.28it/s]WARNING:root:⏱ Timeout en limpieza: How bad does it have to be to be conside\n",
            "\n",
            " 94%|██████████████████████████████████▋  | 46801/49832 [20:09<03:24, 14.79it/s]WARNING:root:⏱ Timeout en limpieza: Personality or hypomania? I feel like ho\n",
            "\n",
            " 95%|███████████████████████████████████▏ | 47458/49832 [20:45<02:17, 17.21it/s]WARNING:root:⏱ Timeout en limpieza: At this point in life I rarely truly wor\n",
            "\n",
            " 95%|███████████████████████████████████▎ | 47505/49832 [20:48<02:08, 18.04it/s]WARNING:root:⏱ Timeout en limpieza: How bad does it have to be to be conside\n",
            "\n",
            " 97%|███████████████████████████████████▊ | 48273/49832 [21:19<00:58, 26.76it/s]WARNING:root:⏱ Timeout en limpieza: Please help me understand what I went th\n",
            "\n",
            " 98%|████████████████████████████████████▍| 49045/49832 [21:50<00:30, 25.48it/s]WARNING:root:⏱ Timeout en limpieza: Backsliding Anxiety Avalanche 35 F and I\n",
            "\n",
            " 99%|████████████████████████████████████▊| 49522/49832 [22:09<00:12, 24.41it/s]WARNING:root:⏱ Timeout en limpieza: Having anxiety and PTSD over being exclu\n",
            "\n",
            "100%|█████████████████████████████████████| 49832/49832 [22:22<00:00, 37.11it/s]\n",
            "💾 Guardando resultados...\n",
            "✅ Archivo guardado como: ./dataset_limpio.csv\n",
            "⏱ Tiempo total: 23.57 minutos\n"
          ]
        }
      ],
      "source": [
        "!python ./resources/scripts/procesar_corpus.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "12ec908f",
      "metadata": {
        "id": "12ec908f"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "url = \"https://drive.google.com/uc?export=download&id=1MfG4qCfqAlj7JBWS0WmLQVOZZvY8wPRf\"\n",
        "r = requests.get(url)\n",
        "\n",
        "# Guarda el contenido descargado en un archivo local\n",
        "with open(\"dataset.csv\", \"wb\") as code:\n",
        "  code.write(r.content)\n",
        "\n",
        "df_cargado = pd.read_csv(\"dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff8870ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff8870ba",
        "outputId": "ab80ab92-44fe-46de-8a22-43386d167f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Tipos después de forzar a listas:\n",
            "clean_tokens\n",
            "<class 'list'>    49710\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import ast\n",
        "\n",
        "def forzar_listas_en_columna(df, columna):\n",
        "    \"\"\"\n",
        "    Convierte todos los valores de la columna en listas reales, si es posible.\n",
        "    Devuelve un error si al final hay valores que no son listas.\n",
        "    \"\"\"\n",
        "    def convertir(x):\n",
        "        if isinstance(x, list):\n",
        "            return x\n",
        "        if isinstance(x, str) and x.strip().startswith(\"[\") and x.strip().endswith(\"]\"):\n",
        "            try:\n",
        "                return ast.literal_eval(x)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error al convertir: {x[:60]}... -> {e}\")\n",
        "                return []\n",
        "        return []\n",
        "\n",
        "    # Aplicar conversión\n",
        "    df[columna] = df[columna].apply(convertir)\n",
        "\n",
        "    # Validación final\n",
        "    tipos_finales = df[columna].apply(type).value_counts()\n",
        "    print(\"\\n📊 Tipos después de forzar a listas:\")\n",
        "    print(tipos_finales)\n",
        "\n",
        "    if any(t != list for t in df[columna].apply(type).unique()):\n",
        "        raise TypeError(\"❌ Aún hay elementos que no son listas en la columna.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Usar en el dataframe cargado\n",
        "df_corregido = forzar_listas_en_columna(df_cargado, \"clean_tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bbd0547",
      "metadata": {
        "id": "5bbd0547"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Entrenamiento del modelo FastText\n",
        "modelo_fasttext = FastText(\n",
        "    sentences=df_corregido['clean_tokens'],         # lista de listas de tokens\n",
        "    vector_size=100,                                # dimensión de los embeddings\n",
        "    window=5,                                       # contexto de palabras\n",
        "    min_count=2,                                    # ignora palabras con frecuencia < 2\n",
        "    sg=1,                                           # usa Skip-gram (1) o CBOW (0)\n",
        "    epochs=10                                       # número de épocas de entrenamiento\n",
        ")\n",
        "\n",
        "# Guardar el modelo para reutilización\n",
        "modelo_fasttext.save(\"modelo_fasttext_gensim.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aa14bd8",
      "metadata": {
        "id": "2aa14bd8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "\n",
        "# Vectoriza un documento utilizando un modelo de palabras\n",
        "def vectorizar_documento(tokens, modelo):\n",
        "    vectores = [modelo.wv[token] for token in tokens if token in modelo.wv]\n",
        "    if vectores:\n",
        "        return np.mean(vectores, axis=0)\n",
        "    else:\n",
        "        return np.zeros(modelo.vector_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce1e2129",
      "metadata": {
        "id": "ce1e2129"
      },
      "outputs": [],
      "source": [
        "# Vectorizar todos los documentos preprocesados\n",
        "X_vectores = np.array([vectorizar_documento(tokens, modelo_fasttext) for tokens in df_cargado['clean_tokens']])\n",
        "\n",
        "# Ver tamaño del resultado\n",
        "print(X_vectores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27aa7fd6",
      "metadata": {
        "id": "27aa7fd6"
      },
      "source": [
        "## **3. Exploración del Embedding**\n",
        "---\n",
        "\n",
        "Puede explorar la representación obtenida por medio de distintas técnicas de visualización o métricas:\n",
        "\n",
        "- **Análisis de Correlaciones**: si tiene una variable objetivo, puede evaluar correlaciones entre los embeddings y dicha variable.\n",
        "- **Nubes de palabras**: puede utilizar gráficos de tipo `wordcloud` para visualizar representaciones basadas en conteos\n",
        "- **Distribuciones**: puede calcular histogramas o gráficos de densidad para mostrar la distribución de embeddings semánticos.\n",
        "\n",
        "_________________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exploración General**\n",
        "En esta sección se muestra algunas métricas resumen del corpus como el tamaño del vocabulario, las palabras más frecuentes, las palabras que presentan similitudes entre sí, similitud entre pares, y  se observa un vector para una palabra elegida.\n"
      ],
      "metadata": {
        "id": "mG4TtEQkXAvK"
      },
      "id": "mG4TtEQkXAvK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver el tamaño del vocabulario:\n",
        "print(f\"Tamaño del vocabulario: {len(modelo_fasttext.wv.key_to_index)}\")"
      ],
      "metadata": {
        "id": "u8HP1qo7VgRc"
      },
      "id": "u8HP1qo7VgRc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver palabras más frecuentes de los documentos\n",
        "palabras_frecuentes = modelo_fasttext.wv.index_to_key[:10]\n",
        "print(\"Palabras más frecuentes:\", palabras_frecuentes)"
      ],
      "metadata": {
        "id": "gag8-WJ3VxWv"
      },
      "id": "gag8-WJ3VxWv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4152f54",
      "metadata": {
        "id": "a4152f54"
      },
      "outputs": [],
      "source": [
        "# Palabras similares a \"happy\"\n",
        "modelo_fasttext.wv.most_similar(\"happy\", topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ed2028",
      "metadata": {
        "id": "13ed2028"
      },
      "outputs": [],
      "source": [
        "# Similitud entre pares\n",
        "print(\"Similitud entre happy y joyful: \" + str(modelo_fasttext.wv.similarity(\"happy\", \"joyful\")))\n",
        "print(\"Similitud entre happy y angry: \" + str(modelo_fasttext.wv.similarity(\"happy\", \"angry\")))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui miramos un ejemplo de un vector para una palabra dentro del corpus\n",
        "palabra_ejemplo = \"people\"\n",
        "if palabra_ejemplo in modelo_fasttext.wv:\n",
        "    print(f\"Vector de '{palabra_ejemplo}': {modelo_fasttext.wv[palabra_ejemplo]}\")\n",
        "else:\n",
        "    print(f\"La palabra '{palabra_ejemplo}' no está en el vocabulario.\")"
      ],
      "metadata": {
        "id": "0DjmsbxLWCUN"
      },
      "id": "0DjmsbxLWCUN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Análisis de Correlaciones**\n",
        "En nuestro corpus de datos, no disponemos de una variable objetivo específica. Esto limita nuestra capacidad para realizar un análisis de correlaciones entre los embeddings y una variable target."
      ],
      "metadata": {
        "id": "5_fpD56PWz-Z"
      },
      "id": "5_fpD56PWz-Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Nubes de palabras**\n",
        "En las nubes de palabras no solamente se observan palabras clave y sus frecuencias, sino que también nos proporcionan una herramienta valiosa para el análisis semántico y la detección de patrones en el lenguaje utilizado en los tweets escritos por los usuarios."
      ],
      "metadata": {
        "id": "4COjT72ZY8aj"
      },
      "id": "4COjT72ZY8aj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c374b280",
      "metadata": {
        "id": "c374b280"
      },
      "outputs": [],
      "source": [
        "# Palabra HAPPY\n",
        "# Genera y muestra una nube de palabras a partir de una lista de palabras\n",
        "def nube_palabras(lista_palabras):\n",
        "    texto = ' '.join(lista_palabras)\n",
        "    nube = WordCloud(width=800, height=400, background_color='white').generate(texto)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(nube, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "# Obtiene las palabras más similares a \"sad\" utilizando el modelo FastText y genera la nube de palabras\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"happy\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Sad\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"sad\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "B0T93oSqaMT7"
      },
      "id": "B0T93oSqaMT7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Life\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"life\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "oajKPCWNckUu"
      },
      "id": "oajKPCWNckUu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Suicide\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"suicide\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "Tokp2AtnZfcU"
      },
      "id": "Tokp2AtnZfcU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Peace\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"peace\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "9bsEw7_TcjfT"
      },
      "id": "9bsEw7_TcjfT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Anxiety\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"anxiety\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "UdIp2zTEaC_y"
      },
      "id": "UdIp2zTEaC_y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Depression\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"depression\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "s-Kpxd1PaJ0t"
      },
      "id": "s-Kpxd1PaJ0t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Joy\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"joy\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "erx3O63faZO7"
      },
      "id": "erx3O63faZO7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribuciones**"
      ],
      "metadata": {
        "id": "-Ib_y4jadGbS"
      },
      "id": "-Ib_y4jadGbS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraemos los embeddings para cada palabra en la columna \"clean_tokens\"\n",
        "df_muestra = df_cargado.sample(frac=0.03) # Usa solo el 10% de los datos\n",
        "embeddings = np.array([modelo_fasttext.wv[word] for tokens in df_muestra[\"clean_tokens\"] for word in tokens if word in modelo_fasttext.wv])\n",
        "\n",
        "\n",
        "# histograma\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(embeddings.flatten(), bins=10, kde=True)\n",
        "plt.title(\"Distribución de Embeddings Semánticos\")\n",
        "plt.xlabel(\"Valor del Embedding\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bolF9zdEdFtp"
      },
      "id": "bolF9zdEdFtp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d902f343",
      "metadata": {
        "id": "d902f343"
      },
      "source": [
        "## **Créditos**\n",
        "\n",
        "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **Diseño de imágenes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}