{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06eb2db5",
      "metadata": {
        "id": "06eb2db5"
      },
      "source": [
        "<img src=\"./resources/images/banner3.png\" width=\"100%\" alt=\"Encabezado MLDS\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb006272",
      "metadata": {
        "id": "fb006272"
      },
      "source": [
        "# **ExtracciÃ³n de CaracterÃ­sticas**\n",
        "---\n",
        "\n",
        "## **0. Integrantes del equipo de trabajo**\n",
        "---\n",
        "\n",
        "<table><thead>\n",
        "  <tr>\n",
        "    <th>#</th>\n",
        "    <th>Integrante</th>\n",
        "    <th>Documento de identidad</th>\n",
        "  </tr></thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Ivonne Cristina Ruiz PÃ¡ez</td>\n",
        "    <td>1014302058</td>  \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>Diego Alejandro Feliciano Ramos</td>\n",
        "    <td>1024586904</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3</td>\n",
        "    <td>Cristhian Enrique CÃ³rdoba Trillos</td>\n",
        "    <td>1030649666</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5bb0b6",
      "metadata": {
        "id": "cd5bb0b6"
      },
      "source": [
        "## **1. SelecciÃ³n del Embedding**\n",
        "---\n",
        "\n",
        "Para el anÃ¡lisis de sentimientos sobre un corpus de 50.000 documentos en inglÃ©s provenientes de redes sociales, se seleccionÃ³ la tÃ©cnica de embedding FastText por su capacidad de generar representaciones distribuidas de palabras que incorporan informaciÃ³n sublÃ©xica a travÃ©s del modelado de n-gramas de caracteres. A diferencia de Word2Vec, que representa cada palabra como un vector independiente, FastText descompone las palabras en subcomponentes, lo cual permite manejar de forma mÃ¡s robusta errores ortogrÃ¡ficos, abreviaciones y palabras fuera del vocabulario (OOV), fenÃ³menos frecuentes en el lenguaje informal y no estructurado caracterÃ­stico de las plataformas sociales. Esta propiedad resulta especialmente Ãºtil para capturar de manera mÃ¡s precisa el contenido semÃ¡ntico de los textos breves y ruidosos. AdemÃ¡s, su eficiencia computacional lo hace adecuado para corpus de tamaÃ±o medio-grande como el presente, permitiendo generar embeddings Ãºtiles para tareas de clasificaciÃ³n sin incurrir en los altos costos de cÃ³mputo de modelos basados en transformers. Si bien FastText no produce embeddings contextuales, su generalizaciÃ³n a partir de subpalabras ofrece una mejora significativa sobre tÃ©cnicas tradicionales como TF-IDF o Bag-of-Words, posicionÃ¡ndolo como una alternativa balanceada en tÃ©rminos de precisiÃ³n y escalabilidad para tareas de anÃ¡lisis de sentimientos en dominios informales como las redes sociales."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fef6dbc",
      "metadata": {
        "id": "3fef6dbc"
      },
      "source": [
        "## **2. ImplementaciÃ³n del Embedding**\n",
        "---\n",
        "\n",
        "Implemente la estrategia de embedding a partir del conjunto de datos pre-procesado. Recuerde que:\n",
        "\n",
        "- `sklearn`: permite implementar bolsas de palabras, TF-IDF y bolsas de N-grams a partir del mÃ³dulo `sklearn.feature_extraction.text`.\n",
        "- `gensim`: permite implementar word2vec, fasttext y doc2vec desde `gensim.models`.\n",
        "- `spacy`: permite representar textos con embeddings pre-entrenados con el atributo `vector`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34dd66cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34dd66cf",
        "outputId": "2694fe23-b4e7-4c8d-860d-c05b61842e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m394.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=792ee4263df5d3941805ac8693a4e328d497a6b54e21194de14ba200155869b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: pyahocorasick, numpy, langdetect, emoji, anyascii, textsearch, scipy, gensim, contractions\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anyascii-0.3.2 contractions-0.1.73 emoji-2.14.1 gensim-4.3.3 langdetect-1.0.9 numpy-1.26.4 pyahocorasick-2.1.0 scipy-1.13.1 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "%pip install textblob kagglehub emoji gensim pandas numpy wordcloud contractions langdetect tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "79zNjXR6JgEV"
      },
      "id": "79zNjXR6JgEV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "630062b1",
      "metadata": {
        "id": "630062b1"
      },
      "outputs": [],
      "source": [
        "# Importa mÃ³dulos para expresiones regulares, manejo de tiempo, registro de eventos, y procesamiento paralelo\n",
        "#\n",
        "import re\n",
        "import time\n",
        "import logging\n",
        "import multiprocessing as mp\n",
        "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
        "\n",
        "# Importa tqdm para mostrar barras de progreso\n",
        "#\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Importa kagglehub para interactuar con Kaggle y pandas para manipulaciÃ³n de datos\n",
        "#\n",
        "import nltk\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "\n",
        "# Importa mÃ³dulos para procesamiento de texto y detecciÃ³n de idioma\n",
        "#\n",
        "from emoji import demojize\n",
        "import contractions\n",
        "from langdetect import detect, DetectorFactory\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import TextBlob\n",
        "from textblob import download_corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53b55fdb",
      "metadata": {
        "id": "53b55fdb"
      },
      "outputs": [],
      "source": [
        "# FunciÃ³n para detectar el idioma de un texto\n",
        "def detectar_idioma(texto):\n",
        "    try:\n",
        "        return detect(str(texto))\n",
        "\n",
        "    except:\n",
        "        return \"unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "981bb032",
      "metadata": {
        "id": "981bb032"
      },
      "outputs": [],
      "source": [
        "# Ruta del archivo CSV de salida\n",
        "OUTPUT_CSV = \"./dataset_limpio.csv\"\n",
        "\n",
        "# Nombre de la columna de texto en el dataset\n",
        "TEXT_COLUMN = \"statement\"\n",
        "\n",
        "# Tiempo mÃ¡ximo de espera en segundos\n",
        "TIMEOUT_SEGUNDOS = 5\n",
        "\n",
        "# Flag opcional para activar correcciÃ³n ortogrÃ¡fica\n",
        "CORREGIR_ORTOGRAFIA = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e15d8f8d",
      "metadata": {
        "id": "e15d8f8d",
        "outputId": "9656e3a9-be70-4731-b567-60d2744d8131"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "def limpiar_texto(texto):\n",
        "    # 0. Asegurar que sea string\n",
        "    texto = str(texto)\n",
        "\n",
        "    # 1. Expandir contracciones (\"I'm\" -> \"I am\")\n",
        "    texto = contractions.fix(texto)\n",
        "\n",
        "    # 2. Demojizar\n",
        "    texto = demojize(texto)\n",
        "\n",
        "    # 3. Reemplazar URLs, menciones, hashtags\n",
        "    texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", texto)\n",
        "    texto = re.sub(r\"@\\w+\", \" \", texto)\n",
        "    texto = re.sub(r\"#(\\w+)\", r\"\\1\", texto)\n",
        "\n",
        "    # 4. Reemplazar puntuaciÃ³n por espacios\n",
        "    texto = re.sub(r\"[^a-zA-Z]\", \" \", texto)\n",
        "\n",
        "    # 5. MinÃºsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # 6. TokenizaciÃ³n\n",
        "    tokens = word_tokenize(texto)\n",
        "\n",
        "    # 7. Filtro de tokens\n",
        "    tokens_filtrados = [\n",
        "        token for token in tokens\n",
        "        if token not in stop_words and len(token) > 1 and token.isalpha()\n",
        "    ]\n",
        "\n",
        "    # 8. CorrecciÃ³n ortogrÃ¡fica opcional\n",
        "    if CORREGIR_ORTOGRAFIA:\n",
        "        tokens_corregidos = []\n",
        "        for token in tokens_filtrados:\n",
        "            palabra_corregida = str(TextBlob(token).correct())\n",
        "            tokens_corregidos.append(palabra_corregida)\n",
        "    else:\n",
        "        tokens_corregidos = tokens_filtrados\n",
        "\n",
        "    # 9. LematizaciÃ³n\n",
        "    tokens_lemmatizados = [lemmatizer.lemmatize(token) for token in tokens_corregidos]\n",
        "\n",
        "    # 10. Retornar lista final de tokens\n",
        "    return tokens_lemmatizados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9631e38",
      "metadata": {
        "id": "d9631e38"
      },
      "outputs": [],
      "source": [
        "# Ejecuta la funciÃ³n de limpieza de texto con un lÃ­mite de tiempo\n",
        "#\n",
        "def timeout_en_limpieza(texto):\n",
        "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
        "        futuro = executor.submit(limpiar_texto, texto)\n",
        "        try:\n",
        "            return futuro.result(timeout=TIMEOUT_SEGUNDOS)\n",
        "        except TimeoutError:\n",
        "            logging.warning(\"â± Timeout en limpieza: \" + str(texto[:40] + \"\\n\"))\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            logging.error(f\"âŒ Error al limpiar texto: {texto[:40]}... -> {str(e)} \\n\")\n",
        "            return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44192db1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44192db1",
        "outputId": "5752542f-d0fa-436e-a152-543da80c4508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/sentiment-analysis-for-mental-health\n"
          ]
        }
      ],
      "source": [
        "# Descarga la Ãºltima versiÃ³n del dataset - Kaggle\n",
        "path = kagglehub.dataset_download(\"suchintikasarkar/sentiment-analysis-for-mental-health\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "# Carga la data del dataset\n",
        "path = '/Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1/Combined Data.csv'\n",
        "df = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efdfe567",
      "metadata": {
        "id": "efdfe567"
      },
      "outputs": [],
      "source": [
        "# Procesa una lista de textos en paralelo utilizando mÃºltiples procesos\n",
        "#\n",
        "def procesar_en_paralelo(funcion, lista_textos, num_procesos=None):\n",
        "    if num_procesos is None:\n",
        "        from os import cpu_count\n",
        "        num_procesos = cpu_count()\n",
        "    with mp.Pool(processes=num_procesos) as pool:\n",
        "        resultados = list(tqdm(pool.imap(funcion, lista_textos), total=len(lista_textos)))\n",
        "    return resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e5bd2b",
      "metadata": {
        "id": "35e5bd2b"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    try:\n",
        "        # Inicia el procesamiento y carga los datos\n",
        "        logging.info(\"ğŸš€ Inicio del procesamiento\")\n",
        "        print(\"ğŸ“¦ Cargando datos...\")\n",
        "        print(f\"âœ… {len(df)} documentos cargados.\")\n",
        "        logging.info(f\"{len(df)} documentos cargados.\")\n",
        "\n",
        "        # Detecta el idioma de los documentos\n",
        "        print(\"ğŸŒ Detectando idioma...\")\n",
        "        df[\"lang\"] = df[TEXT_COLUMN].apply(detectar_idioma)\n",
        "        df_ingles = df[df[\"lang\"] == \"en\"].copy()\n",
        "        print(f\"âœ… Documentos en inglÃ©s: {len(df_ingles)}\")\n",
        "\n",
        "        # Procesa los textos en paralelo con lÃ­mite de tiempo por fila\n",
        "        print(\"ğŸ§  Procesando en paralelo con timeout por fila...\")\n",
        "        textos = df_ingles[TEXT_COLUMN].tolist()\n",
        "        textos_limpios = procesar_en_paralelo(timeout_en_limpieza, textos, num_procesos=11)\n",
        "\n",
        "        # Guarda los resultados en un archivo .CSV\n",
        "        print(\"ğŸ’¾ Guardando resultados...\")\n",
        "        df[\"clean_tokens\"] = textos_limpios\n",
        "        df.to_csv(OUTPUT_CSV, index=False)\n",
        "        logging.info(f\"Archivo guardado como: {OUTPUT_CSV}\")\n",
        "        print(f\"âœ… Archivo guardado como: {OUTPUT_CSV}\")\n",
        "    except Exception as e:\n",
        "        # Maneja errores durante el procesamiento\n",
        "        print(\"âŒ Error durante el procesamiento:\")\n",
        "        print(e)\n",
        "        logging.error(f\"Error general del script: {str(e)}\")\n",
        "    finally:\n",
        "        # Calcula y muestra el tiempo total de procesamiento\n",
        "        end = time.time()\n",
        "        elapsed = (end - start) / 60\n",
        "        print(f\"â± Tiempo total: {elapsed:.2f} minutos\")\n",
        "        logging.info(f\"Tiempo total: {elapsed:.2f} minutos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c04a27b",
      "metadata": {
        "id": "1c04a27b",
        "outputId": "80220ae4-6955-434f-a3e2-aa57a8e42c9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "ğŸ“¦ Cargando datos...\n",
            "âœ… 53043 documentos cargados.\n",
            "ğŸŒ Detectando idioma...\n",
            "âœ… Documentos en inglÃ©s: 49832\n",
            "ğŸ§  Procesando en paralelo con timeout por fila...\n",
            "  0%|                                                 | 0/49832 [00:00<?, ?it/s][nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 5468/49832 [00:45<15:16, 48.41it/s]WARNING:root:â± Timeout en limpieza: 19 years old, male, from the Philippines\n",
            "\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 | 6261/49832 [01:11<25:27, 28.53it/s]WARNING:root:â± Timeout en limpieza: So introduction I guess.. my name is Mic\n",
            "\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 6269/49832 [01:15<1:10:56, 10.23it/s]WARNING:root:â± Timeout en limpieza: I miss my children,They are the purest f\n",
            "\n",
            " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 7893/49832 [02:01<21:31, 32.48it/s]WARNING:root:â± Timeout en limpieza: Hello everyone,I rarely post on Reddit b\n",
            "\n",
            " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 8726/49832 [02:29<25:55, 26.43it/s]WARNING:root:â± Timeout en limpieza: Venting. This is long, sorry. Feel free \n",
            "\n",
            " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 9160/49832 [02:43<22:07, 30.64it/s]WARNING:root:â± Timeout en limpieza: I have only 1 person I can somewhat open\n",
            "\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 9233/49832 [02:51<43:22, 15.60it/s]WARNING:root:â± Timeout en limpieza: Originally posted on the r/suboxone, I t\n",
            "\n",
            " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 9970/49832 [03:08<18:56, 35.07it/s]WARNING:root:â± Timeout en limpieza: I no longer know what else to do but wri\n",
            "\n",
            " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 9978/49832 [03:12<43:48, 15.16it/s]WARNING:root:â± Timeout en limpieza: And has life gotten better?&amp;#x200B;N\n",
            "\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 10231/49832 [03:18<20:08, 32.76it/s]WARNING:root:â± Timeout en limpieza: I am not fucking exaggerating i do not h\n",
            "\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 11409/49832 [03:52<16:07, 39.73it/s]WARNING:root:â± Timeout en limpieza: gross gross gross gross gross gross gros\n",
            "\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 12300/49832 [04:25<43:22, 14.42it/s]WARNING:root:â± Timeout en limpieza: Ok. I am not a short-winded person, so I\n",
            "\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 12523/49832 [04:31<22:14, 27.97it/s]WARNING:root:â± Timeout en limpieza: I wrote a song about how painful it is t\n",
            "\n",
            " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 12785/49832 [04:39<21:27, 28.78it/s]WARNING:root:â± Timeout en limpieza: Hello, before I start, I am not a native\n",
            "\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 13213/49832 [04:53<24:21, 25.06it/s]WARNING:root:â± Timeout en limpieza: Yeah, this is not the first, or even sec\n",
            "\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 14440/49832 [05:28<26:04, 22.62it/s]WARNING:root:â± Timeout en limpieza: it is a terrible idea for one diagnose o\n",
            "\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 16464/49832 [06:28<22:14, 25.01it/s]WARNING:root:â± Timeout en limpieza: You cannot runYou cannot hideSounds oh s\n",
            "\n",
            "WARNING:root:â± Timeout en limpieza: Dear ***,I am writing to you because I h\n",
            "\n",
            "WARNING:root:â± Timeout en limpieza: Pleasekillme. Please. God. ImbeggingyouG\n",
            "\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 16510/49832 [06:34<40:46, 13.62it/s]WARNING:root:â± Timeout en limpieza: Ineededthat. Allthemoreammothatwillhelpm\n",
            "\n",
            "WARNING:root:â± Timeout en limpieza: I am apos thatsleepsoutside behindtheMic\n",
            "\n",
            "WARNING:root:â± Timeout en limpieza: If anyoneseemesleepingbehind theMichaeld\n",
            "\n",
            "WARNING:root:â± Timeout en limpieza: Everynig,ht I hope that when I sle,ep be\n",
            "\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 18065/49832 [07:21<13:58, 37.90it/s]WARNING:root:â± Timeout en limpieza: I am 25 years old. I have struggled with\n",
            "\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 19102/49832 [07:55<14:46, 34.68it/s]WARNING:root:â± Timeout en limpieza: i realised last night that I have been i\n",
            "\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 19947/49832 [08:18<11:21, 43.85it/s]WARNING:root:â± Timeout en limpieza: These are some things that bring me joy.\n",
            "\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 19973/49832 [08:25<38:09, 13.04it/s]WARNING:root:â± Timeout en limpieza: I constantly repeat to myself that I hav\n",
            "\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 20554/49832 [08:36<12:34, 38.82it/s]WARNING:root:â± Timeout en limpieza: I do not expect anyone to read this ramb\n",
            "\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 20750/49832 [08:43<14:44, 32.89it/s]WARNING:root:â± Timeout en limpieza: Do I, a privileged, loved, white woman r\n",
            "\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 21376/49832 [08:59<14:02, 33.78it/s]WARNING:root:â± Timeout en limpieza: For reference, I am a 22 year old you.S \n",
            "\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 22152/49832 [09:22<17:16, 26.72it/s]WARNING:root:â± Timeout en limpieza: I need support or encouragement. I (29M)\n",
            "\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 22158/49832 [09:26<59:13,  7.79it/s]WARNING:root:â± Timeout en limpieza: This is a a vent. I (29M) really do not \n",
            "\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 22537/49832 [09:38<17:05, 26.61it/s]WARNING:root:â± Timeout en limpieza: 7.7.21Should I kill myself?A long journa\n",
            "\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 23108/49832 [09:53<09:57, 44.69it/s]WARNING:root:â± Timeout en limpieza: **Following is a boring and overwhelming\n",
            "\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 23117/49832 [09:58<42:43, 10.42it/s]WARNING:root:â± Timeout en limpieza: **Following is a boring and overwhelming\n",
            "\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 23155/49832 [09:59<29:07, 15.27it/s]WARNING:root:â± Timeout en limpieza: I am feeling like there is no hope in my\n",
            "\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 23579/49832 [10:09<12:52, 34.00it/s]WARNING:root:â± Timeout en limpieza: why even bother going on anymore, it is \n",
            "\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 23790/49832 [10:17<15:24, 28.15it/s]WARNING:root:â± Timeout en limpieza: *Thank you to the moderators of* r/Suici\n",
            "\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 25424/49832 [11:04<17:06, 23.78it/s]WARNING:root:â± Timeout en limpieza: idon't have friends and it hurts being l\n",
            "\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 25515/49832 [11:09<18:17, 22.16it/s]WARNING:root:â± Timeout en limpieza: MARTINWhy are not you talking?GEORGIEBec\n",
            "\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 31990/49832 [12:17<04:22, 67.89it/s]WARNING:root:â± Timeout en limpieza: Pretty bad anxiety towards taking meds w\n",
            "\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 32679/49832 [12:51<15:52, 18.00it/s]WARNING:root:â± Timeout en limpieza: Some very helpful words to put things in\n",
            "\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 32841/49832 [13:00<20:48, 13.61it/s]WARNING:root:â± Timeout en limpieza: Does this appear to be anxiety, or somet\n",
            "\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 36553/49832 [14:34<12:30, 17.69it/s]WARNING:root:â± Timeout en limpieza: hello everyone i m an 9 year old male wi\n",
            "\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 37092/49832 [14:52<05:40, 37.39it/s]WARNING:root:â± Timeout en limpieza: a little over a month ago i wa over at a\n",
            "\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 37219/49832 [14:58<06:36, 31.84it/s]WARNING:root:â± Timeout en limpieza: we ve been seeing a worrying increase in\n",
            "\n",
            "WARNING:root:â± Timeout en limpieza: it doesn t matter anymore i m going to c\n",
            "\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 37219/49832 [15:10<06:36, 31.84it/s]WARNING:root:â± Timeout en limpieza: i miss jesus i miss the warmth warmth of\n",
            "\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 37936/49832 [15:20<04:35, 43.16it/s]WARNING:root:â± Timeout en limpieza: i have come to the conclusion that i am \n",
            "\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 42649/49832 [16:42<10:31, 11.37it/s]WARNING:root:â± Timeout en limpieza: [Rant] I KNEW IT! My medication is poiso\n",
            "\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 43053/49832 [17:09<09:34, 11.79it/s]WARNING:root:â± Timeout en limpieza: as my gujurati jain friend wants scienti\n",
            "\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 43131/49832 [17:14<05:32, 20.18it/s]WARNING:root:â± Timeout en limpieza: Medication Frustration (Triiger warning-\n",
            "\n",
            "WARNING:root:â± Timeout en limpieza: Have been taking Lexapro 10mg for past 4\n",
            "\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 43788/49832 [17:58<09:12, 10.93it/s]WARNING:root:â± Timeout en limpieza: DEPRESSION HAS A PURPOSE: HOW TO USE IT \n",
            "\n",
            "WARNING:root:â± Timeout en limpieza: Creating A Transcendental Platform For T\n",
            "\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 44170/49832 [18:24<04:40, 20.17it/s]WARNING:root:â± Timeout en limpieza: A day in the mixed episode from hell...I\n",
            "\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 44226/49832 [18:29<05:34, 16.77it/s]WARNING:root:â± Timeout en limpieza: A motherâ€™s letter about forgiveness and \n",
            "\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 44817/49832 [18:48<03:19, 25.14it/s]WARNING:root:â± Timeout en limpieza: For anyone on the fence about journaling\n",
            "\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 46183/49832 [19:39<02:23, 25.45it/s]WARNING:root:â± Timeout en limpieza: Stress and your body's endocannabinoid t\n",
            "\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 46649/49832 [19:58<02:30, 21.11it/s]WARNING:root:â± Timeout en limpieza: At this point in life I rarely truly wor\n",
            "\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 46653/49832 [20:01<05:42,  9.28it/s]WARNING:root:â± Timeout en limpieza: How bad does it have to be to be conside\n",
            "\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 46801/49832 [20:09<03:24, 14.79it/s]WARNING:root:â± Timeout en limpieza: Personality or hypomania? I feel like ho\n",
            "\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47458/49832 [20:45<02:17, 17.21it/s]WARNING:root:â± Timeout en limpieza: At this point in life I rarely truly wor\n",
            "\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47505/49832 [20:48<02:08, 18.04it/s]WARNING:root:â± Timeout en limpieza: How bad does it have to be to be conside\n",
            "\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 48273/49832 [21:19<00:58, 26.76it/s]WARNING:root:â± Timeout en limpieza: Please help me understand what I went th\n",
            "\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49045/49832 [21:50<00:30, 25.48it/s]WARNING:root:â± Timeout en limpieza: Backsliding Anxiety Avalanche 35 F and I\n",
            "\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49522/49832 [22:09<00:12, 24.41it/s]WARNING:root:â± Timeout en limpieza: Having anxiety and PTSD over being exclu\n",
            "\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49832/49832 [22:22<00:00, 37.11it/s]\n",
            "ğŸ’¾ Guardando resultados...\n",
            "âœ… Archivo guardado como: ./dataset_limpio.csv\n",
            "â± Tiempo total: 23.57 minutos\n"
          ]
        }
      ],
      "source": [
        "!python ./resources/scripts/procesar_corpus.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "12ec908f",
      "metadata": {
        "id": "12ec908f"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "url = \"https://drive.google.com/uc?export=download&id=1MfG4qCfqAlj7JBWS0WmLQVOZZvY8wPRf\"\n",
        "r = requests.get(url)\n",
        "\n",
        "# Guarda el contenido descargado en un archivo local\n",
        "with open(\"dataset.csv\", \"wb\") as code:\n",
        "  code.write(r.content)\n",
        "\n",
        "df_cargado = pd.read_csv(\"dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff8870ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff8870ba",
        "outputId": "ab80ab92-44fe-46de-8a22-43386d167f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Tipos despuÃ©s de forzar a listas:\n",
            "clean_tokens\n",
            "<class 'list'>    49710\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import ast\n",
        "\n",
        "def forzar_listas_en_columna(df, columna):\n",
        "    \"\"\"\n",
        "    Convierte todos los valores de la columna en listas reales, si es posible.\n",
        "    Devuelve un error si al final hay valores que no son listas.\n",
        "    \"\"\"\n",
        "    def convertir(x):\n",
        "        if isinstance(x, list):\n",
        "            return x\n",
        "        if isinstance(x, str) and x.strip().startswith(\"[\") and x.strip().endswith(\"]\"):\n",
        "            try:\n",
        "                return ast.literal_eval(x)\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Error al convertir: {x[:60]}... -> {e}\")\n",
        "                return []\n",
        "        return []\n",
        "\n",
        "    # Aplicar conversiÃ³n\n",
        "    df[columna] = df[columna].apply(convertir)\n",
        "\n",
        "    # ValidaciÃ³n final\n",
        "    tipos_finales = df[columna].apply(type).value_counts()\n",
        "    print(\"\\nğŸ“Š Tipos despuÃ©s de forzar a listas:\")\n",
        "    print(tipos_finales)\n",
        "\n",
        "    if any(t != list for t in df[columna].apply(type).unique()):\n",
        "        raise TypeError(\"âŒ AÃºn hay elementos que no son listas en la columna.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Usar en el dataframe cargado\n",
        "df_corregido = forzar_listas_en_columna(df_cargado, \"clean_tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bbd0547",
      "metadata": {
        "id": "5bbd0547"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Entrenamiento del modelo FastText\n",
        "modelo_fasttext = FastText(\n",
        "    sentences=df_corregido['clean_tokens'],         # lista de listas de tokens\n",
        "    vector_size=100,                                # dimensiÃ³n de los embeddings\n",
        "    window=5,                                       # contexto de palabras\n",
        "    min_count=2,                                    # ignora palabras con frecuencia < 2\n",
        "    sg=1,                                           # usa Skip-gram (1) o CBOW (0)\n",
        "    epochs=10                                       # nÃºmero de Ã©pocas de entrenamiento\n",
        ")\n",
        "\n",
        "# Guardar el modelo para reutilizaciÃ³n\n",
        "modelo_fasttext.save(\"modelo_fasttext_gensim.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aa14bd8",
      "metadata": {
        "id": "2aa14bd8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "\n",
        "# Vectoriza un documento utilizando un modelo de palabras\n",
        "def vectorizar_documento(tokens, modelo):\n",
        "    vectores = [modelo.wv[token] for token in tokens if token in modelo.wv]\n",
        "    if vectores:\n",
        "        return np.mean(vectores, axis=0)\n",
        "    else:\n",
        "        return np.zeros(modelo.vector_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce1e2129",
      "metadata": {
        "id": "ce1e2129"
      },
      "outputs": [],
      "source": [
        "# Vectorizar todos los documentos preprocesados\n",
        "X_vectores = np.array([vectorizar_documento(tokens, modelo_fasttext) for tokens in df_cargado['clean_tokens']])\n",
        "\n",
        "# Ver tamaÃ±o del resultado\n",
        "print(X_vectores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27aa7fd6",
      "metadata": {
        "id": "27aa7fd6"
      },
      "source": [
        "## **3. ExploraciÃ³n del Embedding**\n",
        "---\n",
        "\n",
        "Puede explorar la representaciÃ³n obtenida por medio de distintas tÃ©cnicas de visualizaciÃ³n o mÃ©tricas:\n",
        "\n",
        "- **AnÃ¡lisis de Correlaciones**: si tiene una variable objetivo, puede evaluar correlaciones entre los embeddings y dicha variable.\n",
        "- **Nubes de palabras**: puede utilizar grÃ¡ficos de tipo `wordcloud` para visualizar representaciones basadas en conteos\n",
        "- **Distribuciones**: puede calcular histogramas o grÃ¡ficos de densidad para mostrar la distribuciÃ³n de embeddings semÃ¡nticos.\n",
        "\n",
        "_________________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ExploraciÃ³n General**\n",
        "En esta secciÃ³n se muestra algunas mÃ©tricas resumen del corpus como el tamaÃ±o del vocabulario, las palabras mÃ¡s frecuentes, las palabras que presentan similitudes entre sÃ­, similitud entre pares, y  se observa un vector para una palabra elegida.\n"
      ],
      "metadata": {
        "id": "mG4TtEQkXAvK"
      },
      "id": "mG4TtEQkXAvK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver el tamaÃ±o del vocabulario:\n",
        "print(f\"TamaÃ±o del vocabulario: {len(modelo_fasttext.wv.key_to_index)}\")"
      ],
      "metadata": {
        "id": "u8HP1qo7VgRc"
      },
      "id": "u8HP1qo7VgRc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver palabras mÃ¡s frecuentes de los documentos\n",
        "palabras_frecuentes = modelo_fasttext.wv.index_to_key[:10]\n",
        "print(\"Palabras mÃ¡s frecuentes:\", palabras_frecuentes)"
      ],
      "metadata": {
        "id": "gag8-WJ3VxWv"
      },
      "id": "gag8-WJ3VxWv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4152f54",
      "metadata": {
        "id": "a4152f54"
      },
      "outputs": [],
      "source": [
        "# Palabras similares a \"happy\"\n",
        "modelo_fasttext.wv.most_similar(\"happy\", topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ed2028",
      "metadata": {
        "id": "13ed2028"
      },
      "outputs": [],
      "source": [
        "# Similitud entre pares\n",
        "print(\"Similitud entre happy y joyful: \" + str(modelo_fasttext.wv.similarity(\"happy\", \"joyful\")))\n",
        "print(\"Similitud entre happy y angry: \" + str(modelo_fasttext.wv.similarity(\"happy\", \"angry\")))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui miramos un ejemplo de un vector para una palabra dentro del corpus\n",
        "palabra_ejemplo = \"people\"\n",
        "if palabra_ejemplo in modelo_fasttext.wv:\n",
        "    print(f\"Vector de '{palabra_ejemplo}': {modelo_fasttext.wv[palabra_ejemplo]}\")\n",
        "else:\n",
        "    print(f\"La palabra '{palabra_ejemplo}' no estÃ¡ en el vocabulario.\")"
      ],
      "metadata": {
        "id": "0DjmsbxLWCUN"
      },
      "id": "0DjmsbxLWCUN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **AnÃ¡lisis de Correlaciones**\n",
        "En nuestro corpus de datos, no disponemos de una variable objetivo especÃ­fica. Esto limita nuestra capacidad para realizar un anÃ¡lisis de correlaciones entre los embeddings y una variable target."
      ],
      "metadata": {
        "id": "5_fpD56PWz-Z"
      },
      "id": "5_fpD56PWz-Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Nubes de palabras**\n",
        "En las nubes de palabras no solamente se observan palabras clave y sus frecuencias, sino que tambiÃ©n nos proporcionan una herramienta valiosa para el anÃ¡lisis semÃ¡ntico y la detecciÃ³n de patrones en el lenguaje utilizado en los tweets escritos por los usuarios."
      ],
      "metadata": {
        "id": "4COjT72ZY8aj"
      },
      "id": "4COjT72ZY8aj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c374b280",
      "metadata": {
        "id": "c374b280"
      },
      "outputs": [],
      "source": [
        "# Palabra HAPPY\n",
        "# Genera y muestra una nube de palabras a partir de una lista de palabras\n",
        "def nube_palabras(lista_palabras):\n",
        "    texto = ' '.join(lista_palabras)\n",
        "    nube = WordCloud(width=800, height=400, background_color='white').generate(texto)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(nube, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "# Obtiene las palabras mÃ¡s similares a \"sad\" utilizando el modelo FastText y genera la nube de palabras\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"happy\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Sad\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"sad\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "B0T93oSqaMT7"
      },
      "id": "B0T93oSqaMT7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Life\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"life\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "oajKPCWNckUu"
      },
      "id": "oajKPCWNckUu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Suicide\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"suicide\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "Tokp2AtnZfcU"
      },
      "id": "Tokp2AtnZfcU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Peace\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"peace\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "9bsEw7_TcjfT"
      },
      "id": "9bsEw7_TcjfT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Anxiety\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"anxiety\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "UdIp2zTEaC_y"
      },
      "id": "UdIp2zTEaC_y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Depression\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"depression\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "s-Kpxd1PaJ0t"
      },
      "id": "s-Kpxd1PaJ0t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabra Joy\n",
        "palabras_similares = [w for w, _ in modelo_fasttext.wv.most_similar(\"joy\", topn=50)]\n",
        "nube_palabras(palabras_similares)"
      ],
      "metadata": {
        "id": "erx3O63faZO7"
      },
      "id": "erx3O63faZO7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribuciones**"
      ],
      "metadata": {
        "id": "-Ib_y4jadGbS"
      },
      "id": "-Ib_y4jadGbS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraemos los embeddings para cada palabra en la columna \"clean_tokens\"\n",
        "df_muestra = df_cargado.sample(frac=0.03) # Usa solo el 10% de los datos\n",
        "embeddings = np.array([modelo_fasttext.wv[word] for tokens in df_muestra[\"clean_tokens\"] for word in tokens if word in modelo_fasttext.wv])\n",
        "\n",
        "\n",
        "# histograma\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(embeddings.flatten(), bins=10, kde=True)\n",
        "plt.title(\"DistribuciÃ³n de Embeddings SemÃ¡nticos\")\n",
        "plt.xlabel(\"Valor del Embedding\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bolF9zdEdFtp"
      },
      "id": "bolF9zdEdFtp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d902f343",
      "metadata": {
        "id": "d902f343"
      },
      "source": [
        "## **CrÃ©ditos**\n",
        "\n",
        "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan SebastiÃ¡n Lara RamÃ­rez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **DiseÃ±o de imÃ¡genes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "* **Coordinador de virtualizaciÃ³n:**\n",
        "    - [Edder HernÃ¡ndez Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de IngenierÃ­a*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}