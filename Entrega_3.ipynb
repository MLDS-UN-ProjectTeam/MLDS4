{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06eb2db5",
      "metadata": {
        "id": "06eb2db5"
      },
      "source": [
        "<img src=\"./resources/images/banner3.png\" width=\"100%\" alt=\"Encabezado MLDS\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb006272",
      "metadata": {
        "id": "fb006272"
      },
      "source": [
        "# **Extracción de Características**\n",
        "---\n",
        "\n",
        "## **0. Integrantes del equipo de trabajo**\n",
        "---\n",
        "\n",
        "<table><thead>\n",
        "  <tr>\n",
        "    <th>#</th>\n",
        "    <th>Integrante</th>\n",
        "    <th>Documento de identidad</th>\n",
        "  </tr></thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Ivonne Cristina Ruiz Páez</td>\n",
        "    <td>1014302058</td>  \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>Diego Alejandro Feliciano Ramos</td>\n",
        "    <td>1024586904</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3</td>\n",
        "    <td>Cristhian Enrique Córdoba Trillos</td>\n",
        "    <td>1030649666</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5bb0b6",
      "metadata": {
        "id": "cd5bb0b6"
      },
      "source": [
        "## **1. Selección del Embedding**\n",
        "---\n",
        "\n",
        "Para el análisis de sentimientos sobre un corpus de 50.000 documentos en inglés provenientes de redes sociales, se seleccionó la técnica de embedding FastText por su capacidad de generar representaciones distribuidas de palabras que incorporan información subléxica a través del modelado de n-gramas de caracteres. A diferencia de Word2Vec, que representa cada palabra como un vector independiente, FastText descompone las palabras en subcomponentes, lo cual permite manejar de forma más robusta errores ortográficos, abreviaciones y palabras fuera del vocabulario (OOV), fenómenos frecuentes en el lenguaje informal y no estructurado característico de las plataformas sociales. Esta propiedad resulta especialmente útil para capturar de manera más precisa el contenido semántico de los textos breves y ruidosos. Además, su eficiencia computacional lo hace adecuado para corpus de tamaño medio-grande como el presente, permitiendo generar embeddings útiles para tareas de clasificación sin incurrir en los altos costos de cómputo de modelos basados en transformers. Si bien FastText no produce embeddings contextuales, su generalización a partir de subpalabras ofrece una mejora significativa sobre técnicas tradicionales como TF-IDF o Bag-of-Words, posicionándolo como una alternativa balanceada en términos de precisión y escalabilidad para tareas de análisis de sentimientos en dominios informales como las redes sociales."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fef6dbc",
      "metadata": {
        "id": "3fef6dbc"
      },
      "source": [
        "## **2. Implementación del Embedding**\n",
        "---\n",
        "\n",
        "Implemente la estrategia de embedding a partir del conjunto de datos pre-procesado. Recuerde que:\n",
        "\n",
        "- `sklearn`: permite implementar bolsas de palabras, TF-IDF y bolsas de N-grams a partir del módulo `sklearn.feature_extraction.text`.\n",
        "- `gensim`: permite implementar word2vec, fasttext y doc2vec desde `gensim.models`.\n",
        "- `spacy`: permite representar textos con embeddings pre-entrenados con el atributo `vector`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "34dd66cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: textblob in /Users/diegof/Library/Python/3.9/lib/python/site-packages (0.19.0)\n",
            "Requirement already satisfied: kagglehub in /Users/diegof/Library/Python/3.9/lib/python/site-packages (0.3.11)\n",
            "Requirement already satisfied: emoji in /Users/diegof/Library/Python/3.9/lib/python/site-packages (2.14.1)\n",
            "Requirement already satisfied: gensim in /Users/diegof/Library/Python/3.9/lib/python/site-packages (4.3.3)\n",
            "Requirement already satisfied: pandas in /Users/diegof/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in /Users/diegof/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
            "Requirement already satisfied: nltk>=3.9 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: pyyaml in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: packaging in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: joblib in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: click in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
            "Requirement already satisfied: wrapt in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from requests->kagglehub) (2025.1.31)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install textblob kagglehub emoji gensim pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e15d8f8d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/diegof/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from emoji import demojize\n",
        "\n",
        "# Descargar recursos de NLTK (solo la primera vez)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Instanciar herramientas\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Función mejorada de limpieza, normalización y tokenización\n",
        "def limpiar_texto(texto):\n",
        "    # Convertir a string por seguridad\n",
        "    texto = str(texto)\n",
        "\n",
        "    # 1. Normalización: convertir a minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # 2. Limpieza con Regex\n",
        "    texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', texto)  # eliminar links\n",
        "    texto = re.sub(r'@\\w+', '', texto)                     # eliminar menciones\n",
        "    texto = re.sub(r'#(\\w+)', r'\\1', texto)                # eliminar '#' pero conservar palabra del hashtag\n",
        "    texto = re.sub(r'[^a-zA-Z\\s]', '', texto)              # eliminar caracteres especiales y números\n",
        "\n",
        "    # 3. Tokenización básica\n",
        "    tokens = word_tokenize(texto)\n",
        "\n",
        "    # 4. Filtrar stopwords y tokens muy cortos\n",
        "    tokens_filtrados = [\n",
        "        token for token in tokens\n",
        "        if token not in stop_words and len(token) > 1\n",
        "    ]\n",
        "\n",
        "    # 5. Reemplazar emojis por su representación textual\n",
        "    tokens_demojizados = [demojize(token) for token in tokens_filtrados]\n",
        "\n",
        "    # 6. Separar ':' de los emojis textualizados para evitar que queden como un solo token\n",
        "    tokens_ajustados = []\n",
        "    for token in tokens_demojizados:\n",
        "        if token.startswith(':') and token.endswith(':'):\n",
        "            token = token.replace(\":\", \" \")  # convertir :smile: → \" smile, por ejemplo \"\n",
        "            token = token.strip()\n",
        "            tokens_ajustados.extend(token.split())\n",
        "        else:\n",
        "            tokens_ajustados.append(token)\n",
        "\n",
        "    # 7. Lematización (transformar palabras a su raíz)\n",
        "    tokens_lemmatizados = [lemmatizer.lemmatize(token) for token in tokens_ajustados]\n",
        "\n",
        "    # 8. Retornar lista de tokens (ideal para modelos como FastText en Gensim)\n",
        "    return tokens_lemmatizados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "44192db1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/diegof/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/diegof/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import kagglehub\n",
        "\n",
        "# Descarga la última versión del dataset\n",
        "path = kagglehub.dataset_download(\"suchintikasarkar/sentiment-analysis-for-mental-health\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "# Carga la data del dataset\n",
        "path = '/Users/diegof/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1/Combined Data.csv'\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Aplica preprocesamiento a todos los textos\n",
        "corpus_tokenizado = df['statement'].apply(limpiar_texto).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "50082b16",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: scipy==1.10.1 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/diegof/Library/Python/3.9/lib/python/site-packages (from scipy==1.10.1) (1.26.4)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install scipy==1.10.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5bbd0547",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Entrenamiento del modelo FastText\n",
        "modelo_fasttext = FastText(\n",
        "    sentences=corpus_tokenizado,  # lista de listas de tokens\n",
        "    vector_size=100,              # dimensión de los embeddings\n",
        "    window=5,                     # contexto de palabras\n",
        "    min_count=2,                  # ignora palabras con frecuencia < 2\n",
        "    sg=1,                         # usa Skip-gram (1) o CBOW (0)\n",
        "    epochs=10                     # número de épocas de entrenamiento\n",
        ")\n",
        "\n",
        "# Guardar el modelo si quieres reutilizarlo\n",
        "modelo_fasttext.save(\"modelo_fasttext_gensim.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2aa14bd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorizar_documento(tokens, modelo):\n",
        "    vectores = [modelo.wv[token] for token in tokens if token in modelo.wv]\n",
        "    if vectores:\n",
        "        return np.mean(vectores, axis=0)\n",
        "    else:\n",
        "        return np.zeros(modelo.vector_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce1e2129",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(53043, 100)\n"
          ]
        }
      ],
      "source": [
        "# Vectorizar todos los documentos preprocesados\n",
        "X_vectores = np.array([vectorizar_documento(tokens, modelo_fasttext) for tokens in corpus_tokenizado])\n",
        "\n",
        "# Ver tamaño del resultado\n",
        "print(X_vectores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27aa7fd6",
      "metadata": {
        "id": "27aa7fd6"
      },
      "source": [
        "## **3. Exploración del Embedding**\n",
        "---\n",
        "\n",
        "Puede explorar la representación obtenida por medio de distintas técnicas de visualización o métricas:\n",
        "\n",
        "- **Análisis de Correlaciones**: si tiene una variable objetivo, puede evaluar correlaciones entre los embeddings y dicha variable.\n",
        "- **Nubes de palabras**: puede utilizar gráficos de tipo `wordcloud` para visualizar representaciones basadas en conteos\n",
        "- **Distribuciones**: puede calcular histogramas o gráficos de densidad para mostrar la distribución de embeddings semánticos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d902f343",
      "metadata": {
        "id": "d902f343"
      },
      "source": [
        "## **Créditos**\n",
        "\n",
        "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **Diseño de imágenes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
